# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z9ySyCt04bP06nV3pQMeDEX7HVmK-smA
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, mean_absolute_error , mean_squared_error, r2_score, accuracy_score, confusion_matrix, precision_score, recall_score, f1_score
from sklearn.preprocessing import StandardScaler
import joblib
import kagglehub
import os

path= kagglehub.dataset_download("ajinilpatel/energy-consumption-prediction")
print("Path to dataset files:", path)
#kaggle repo: https://www.kaggle.com/datasets/ajinilpatel/energy-consumption-prediction/data

#define path to file
os.listdir(path)
file_path=os.path.join(path, "Energy_consumption_dataset.csv")
print(file_path)

#load dataset as dataframe
df_main=pd.read_csv(file_path)
df_main.info()
df_main.head()

#create a working copy to manipulate while maintaining original datafram
df=df_main.copy()
#drop columns with more than half of their entries as Null
df=df.loc[:, df.isna().mean() < 0.5]
#drop records with null values in the Energy Consumption column
df=df.dropna(subset=["EnergyConsumption"])
#cast the Energy Consumption column as a float type for numerical analysis
y=df["EnergyConsumption"].astype(float)
#X= df.drop(columns=[col for col in ["EnergyConsumption", "EnergyBin"]if col in df.columns])
#drop the Energy Consumption column from the base frame to train AI with
X=df.drop(columns=["EnergyConsumption"])
#make words binary
X=pd.get_dummies(X, drop_first=True)
print(f"X shape: {X.shape}")
feature_name=X.columns.tolist()

def two_stage_split(X, y, holdout_frac=0.2, test_frac_within=0.2, random_state=42):
  X_hold, X_rest, y_hold, y_rest = train_test_split(X, y, test_size=1-holdout_frac, random_state=random_state)
  X_train, X_test, y_train, y_test = train_test_split(X_rest, y_rest, test_size=test_frac_within, random_state=random_state)
  return (X_train, X_test, X_hold, y_train, y_test, y_hold)

#function to domicile regression
def report_regression(y_true, y_pred, prefix=''):
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = mse ** 0.5
    r2 = r2_score(y_true, y_pred)
    print(f"{prefix} MAE: {mae:.4f}")
    print(f"{prefix} MSE: {mse:.4f}")
    print(f"{prefix} RMSE: {rmse:.4f}")
    print(f"{prefix} R^2: {r2:.4f}")
    return {'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2}

def show_cv_scores(model, X, y, cv=5, scoring='neg_mean_squared_error'):
    scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)
    if scoring.startswith('neg_'):
        scores = -scores
    print(f"CV ({scoring}) mean: {scores.mean():.4f}, std: {scores.std():.4f}")
    return scores

#Split data
X_train, X_test, X_hold, y_train, y_test, y_hold = two_stage_split(X.values, y.values, holdout_frac=0.2, test_frac_within=0.2, random_state=42)

scaler= StandardScaler()
X_train_sc= scaler.fit_transform(X_train)
X_test_sc= scaler.transform(X_test)
X_hold_sc= scaler.transform(X_hold)

#linear regression
rgr= LinearRegression()
rgr.fit(X_train_sc, y_train)
y_pred_test= rgr.predict(X_test_sc)
print('LinearRegression on test set:')
report_regression(y_test, y_pred_test, prefix='Test')
print('\nLinearRegression cross-validation (on training set):')
show_cv_scores(rgr, X_train_sc, y_train, cv=5, scoring='neg_mean_squared_error')

y_hold_test=rgr.predict(X_hold_sc)
print("LinearRegression on hold out set:")
report_regression(y_hold, y_hold_test, prefix="Holdout")

#Random Forest
rnf=RandomForestRegressor(n_estimators=100)
rnf.fit(X_train, y_train)
y_pred_test_rnf= rnf.predict(X_test)
print("Random Forest on test set: ")
report_regression(y_test, y_pred_test_rnf, prefix="Test")
print("\nRandom Forest cross-validation (on training set):")
show_cv_scores(rnf, X_train, y_train, cv=5, scoring="neg_mean_squared_error")

y_hold_test_rnf= rnf.predict(X_hold)
print("Random Forest on hold out set: ")
report_regression(y_hold, y_hold_test_rnf, prefix="Holdout")
show_cv_scores(rnf, X_hold, y_hold, cv=5, scoring="neg_mean_squared_error")

df_class=df.copy()
bin_ends=np.arange(0, df_class["EnergyConsumption"].max()+5, 5)
df_class["EnergyBin"] = pd.cut(df_class["EnergyConsumption"], bins=bin_ends, labels=False)
Xc= df_class.drop(columns=["EnergyConsumption", "EnergyBin"])
yc= df_class["EnergyBin"]
Xc=pd.get_dummies(Xc, drop_first=True)
X_train, X_test, X_hold, y_train, y_test, y_hold = two_stage_split(Xc.values, yc.values, holdout_frac=0.2, test_frac_within=0.2, random_state=42)
clf= RandomForestClassifier(n_estimators=100)
clf.fit(X_train, y_train)
y_pred_test_clf= clf.predict(X_test)
print("Random Forest Classifier on test set: ")
print(classification_report(y_test, y_pred_test_clf))